{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import sys\n",
    "#for time generator\n",
    "from datetime import timedelta, date, datetime\n",
    "from datetime import time as time_\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_reddit_submission_data(from_time_interval):\n",
    "    '''\n",
    "    using praw search for all submissions created within this time period.\n",
    "    returns a list of submissions(each likely being a dictionary)\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_reddit_connection():\n",
    "    \n",
    "    try:\n",
    "        from reddit_config import client_id,password,user_agent,username\n",
    "        print('reddit oath credentials successfully loaded')\n",
    "    except Exception as e:\n",
    "        print('ERROR-0')\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "    reddit = praw.Reddit(client_id=client_id,\n",
    "                        client_secret=password,\n",
    "                        user_agent=user_agent,\n",
    "                        username=username)\n",
    "    return reddit\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def insert_into_database(reddit_submission_raw_data):\n",
    "    '''\n",
    "    this function will update the database and append this new row to it\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def get_db():\n",
    "    \"\"\"\n",
    "    gets db, creates one if it does not exist\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_date_object(date_string):\n",
    "    '''\n",
    "    transforms a string from 'month/day/year' and returns equivalent date(month,day,year) object\n",
    "    '''\n",
    "    assert len(date_string) == 10\n",
    "    date_comp = date_string.split('/')\n",
    "    year = int(date_comp[2])\n",
    "    month= int(date_comp[0])\n",
    "    day = int(date_comp[1])\n",
    "    \n",
    "    return date(year,month,day)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    '''\n",
    "    creates a generator that lets you iterate through a date range, one day at a time. Does not include the\n",
    "    last date day just like range function.\n",
    "    \n",
    "    e.g:\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "    '''\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_to_csv(submission_data):\n",
    "    \"\"\"\n",
    "    saves list of submission data to a local csv file\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    import csv\n",
    "    \n",
    "    time_now = str(datetime.datetime.now())[:-7]\n",
    "    time_interval = submission_data[0] + '-' + submission_data[-1]\n",
    "    \n",
    "    \n",
    "    with open(\"{}-{}.csv\".format(time_interval,time_now, \"w\")) as output:\n",
    "        w = csv.DictWriter(output,submission_data[0].keys())\n",
    "        for row_dict in submission_data:\n",
    "              w.writeheader()\n",
    "              w.writerow(row_dict)\n",
    "        \n",
    "    \n",
    "    pass\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def collect_logging():\n",
    "    \"\"\"\n",
    "    logs any errors\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    sys.exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loads list of all nba subreddits to parse from 'nba_reddit_subnames.csv'\n",
    "\n",
    "subreddits_list = []\n",
    "with open('nba_reddit_subnames.csv',newline='') as f:\n",
    "    r = csv.reader(f)\n",
    "    for row in r:\n",
    "        subreddits_list.append([row[0],row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###contains NBA season start and end dates for each seasons offseason, regular season and playoffs\n",
    "###the end date is incremented by one day\n",
    "\n",
    "season_dates = {'2013': {'offseason' : ('06/21/2013','11/28/2013'),'reg_season':('11/29/2013','04/16/2014'),'playoffs':('04/19/2014','06/15/2014')}\n",
    ", \n",
    "'2014': {'offseason' : ('06/21/2014','11/28/2014'),'reg_season':('10/28/2014','04/15/2015'),'playoffs':('04/18/2015','06/16/2015')}\n",
    ",\n",
    "'2015': {'offseason' : ('06/17/2015','10/26/2015'),'reg_season':('10/27/2015','04/13/2016'),'playoffs':('04/16/2016','06/19/2016')} \n",
    ",\n",
    "'2016' : {'offseason' : ('06/20/2016','10/24/2016'),'reg_season':('10/25/2016','04/12/2017'),'playoffs':('04/15/2017','06/12/2017')}\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "subreddits_list = list of 2 item lists that contains shorthand city name /r/<subname>. ['LAL','lakers']\n",
    "\n",
    "date_range = tuple of date objects\n",
    "\n",
    "def reddit_Submissions_toJSON(subreddits_list,date_range):\n",
    "    ([list of ['LAL','lakers']],(start_dateobject,end_dateobject)) -> returns JSON data for all submissions in this period\n",
    "    \n",
    "    \n",
    "reddit_Submissions_toJSON(subreddits_list,season_dates['2014]['playoffs])\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#load oauth credentials and create reddit instance\n",
    "from reddit_config import client_id,password,user_agent,username\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                        client_secret=password,\n",
    "                        user_agent=user_agent,\n",
    "                        username=username)\n",
    "\n",
    "#start_date = to_date_object(date_range[0])\n",
    "#end_date = to_date_object(date_range[1])\n",
    "#start day =  first day of playoffs\n",
    "#end date = last day of playoffs\n",
    "start_date = to_date_object(season_dates['2014']['playoffs'][0])\n",
    "end_date = to_date_object(season_dates['2014']['playoffs'][1])\n",
    "type_season = 'playoffs'\n",
    "nba_yr = '2014'\n",
    "\n",
    "\n",
    "program_start_time = time.time()\n",
    "\n",
    "\n",
    "json_totalseason = {'data_generated_date':datetime.now().strftime(\"%a, %d %b %Y %H:%M:%S -0800\"),\n",
    "                    'period_data':[],'runtime':''}\n",
    "#for each day during the playoffs\n",
    "for single_day in daterange(start_date,end_date):\n",
    "    day_start = time.mktime(datetime.combine(single_day,time_(0,0,0)).timetuple())\n",
    "    day_end = time.mktime(datetime.combine(single_day,time_(23,59,59)).timetuple())\n",
    "    \n",
    "    #this dicts holds data for the total day. Each day is stored as a list of dicts under 'data' key\n",
    "    #date is date for the day of data, day_count is total # of submissions across all nba subreddits\n",
    "    #errors logs any days for any subreddits where the data extraction process failed\n",
    "    json_totalday = {'date':'','day_count':0,'data':[],'errors':[]}\n",
    "    #counter for total submissions per day\n",
    "    total_subs = 0\n",
    "    for team in subreddits_list:\n",
    "        #initializes our error catcher for the day\n",
    "        json_error_catcher = []\n",
    "        #get the /r/subredditname and pass it to praw reddit object\n",
    "        subreddit = reddit.subreddit(team[1])\n",
    "        \n",
    "        #if this try fails that means data extraction failed for a particular day, so log the day and subreddit\n",
    "        try:\n",
    "            #for all submissions for 1 subreddit from start of day to end\n",
    "            for submission in subreddit.submissions(day_start,day_end):\n",
    "                #json_row holds our row data\n",
    "                json_row = {'title':None,\n",
    "                            'subreddit':None,\n",
    "                            'author':None,\n",
    "                            'create_date':None,\n",
    "                            'submission_id':None,\n",
    "                            'num_comments':None}\n",
    "                \n",
    "                #increment our total sub counter\n",
    "                total_subs+=1\n",
    "                \n",
    "                #get all the data values wanted\n",
    "                json_row['title']=submission.title\n",
    "                json_row['subreddit']=team[0]\n",
    "                json_row['author'] = submission.author.name\n",
    "                json_row['create_date'] = submission.created_utc\n",
    "                json_row['submission_id'] = submission.id\n",
    "                json_row['num_comments'] = submission.num_comments\n",
    "                \n",
    "                #appends the json data row to list in totalday data. If extraction failed the values will be None    \n",
    "                json_totalday['data'].append(json_row)\n",
    "\n",
    "                \n",
    "        except:\n",
    "            #if extraction fails, create a list of 2 items, the string date and string name for subreddit\n",
    "            #and append it to json_error_catcher list\n",
    "            json_error = {'error':True,'team':team[0],'day':single_day.strftime('%Y-%m-%d')}\n",
    "            json_totalday['data'].append(json_error)\n",
    "            json_error_catcher.append([single_day.strftime('%Y-%m-%d'),team[0]])\n",
    "            \n",
    "            \n",
    "        \n",
    "    #records the date\n",
    "    json_totalday['date'] = single_day.strftime('%Y-%m-%d')\n",
    "    #records total subs for day\n",
    "    json_totalday['day_count'] = total_subs\n",
    "    \n",
    "    #if there was an error then append the errors to the 'errors' list in json_totalday for later inspection\n",
    "    #if no errors then json_totalday['errors] will be an empty list.\n",
    "    if len(json_error_catcher) > 1: json_totalday['errors'].append(json_row['error'])\n",
    "    \n",
    "    \n",
    "    json_totalseason['period_data'].append(json_totalday)\n",
    "    print(single_day.strftime('%m-%d'),'done')\n",
    "\n",
    "runtime = time.time() - program_start_time\n",
    "json_totalseason['runtime']=runtime\n",
    "print('COMPLETE')     \n",
    "print(\"--- %s seconds ---\" % (runtime)\n",
    "\n",
    "'''\n",
    "import json\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(json_totalseason, outfile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "json_totalseason['period_data'][0]['errors']\n",
    "\n",
    "for day in json_totalseason['period_data']:\n",
    "    team_count = {}\n",
    "    print(day['date'],day['day_count'])\n",
    "    for row in day['data']:\n",
    "        team_count[row['subreddit']] = team_count.get(row['subreddit'],0) + 1\n",
    "    \n",
    "    print(sum(team_count.values()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "subreddits_list = list of 2 item lists that contains shorthand city name /r/<subname>. ['LAL','lakers']\n",
    "\n",
    "date_range = tuple of date objects\n",
    "\n",
    "def reddit_Submissions_toJSON(subreddits_list,date_range):\n",
    "    ([list of ['LAL','lakers']],(start_dateobject,end_dateobject)) -> returns JSON data for all submissions in this period\n",
    "    \n",
    "    \n",
    "reddit_Submissions_toJSON(subreddits_list,season_dates['2014]['playoffs])\n",
    "\n",
    "\n",
    "'''\n",
    "def reddit_Submissions_toJSON(subreddits_list,date_range):\n",
    "    '''\n",
    "    (list,tuple) -> \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #load oauth credentials and create reddit instance\n",
    "    from reddit_config import client_id,password,user_agent,username\n",
    "    reddit = praw.Reddit(client_id=client_id,\n",
    "                            client_secret=password,\n",
    "                            user_agent=user_agent,\n",
    "                            username=username)\n",
    "\n",
    "    \n",
    "    #start day =  first day of playoffs\n",
    "    #end date = last day of playoffs\n",
    "    start_date = to_date_object(date_range[0])\n",
    "    end_date = to_date_object(date_range[1])\n",
    "\n",
    "    \n",
    "    program_start_time = time.time()\n",
    "\n",
    "\n",
    "    json_totalseason = {'data_generated_date':datetime.now().strftime(\"%a, %d %b %Y %H:%M:%S -0800\"),\n",
    "                        'period_data':[],'runtime':''}\n",
    "    #for each day during the playoffs\n",
    "    for single_day in daterange(start_date,end_date):\n",
    "        day_start = time.mktime(datetime.combine(single_day,time_(0,0,0)).timetuple())\n",
    "        day_end = time.mktime(datetime.combine(single_day,time_(23,59,59)).timetuple())\n",
    "\n",
    "        #this dicts holds data for the total day. Each day is stored as a list of dicts under 'data' key\n",
    "        #date is date for the day of data, day_count is total # of submissions across all nba subreddits\n",
    "        #errors logs any days for any subreddits where the data extraction process failed\n",
    "        json_totalday = {'date':'','day_count':0,'data':[],'errors':[]}\n",
    "        #counter for total submissions per day\n",
    "        total_subs = 0\n",
    "        for team in subreddits_list:\n",
    "            #initializes our error catcher for the day\n",
    "            json_error_catcher = []\n",
    "            #get the /r/subredditname and pass it to praw reddit object\n",
    "            subreddit = reddit.subreddit(team[1])\n",
    "\n",
    "            #if this try fails that means data extraction failed for a particular day, so log the day and subreddit\n",
    "            try:\n",
    "                #for all submissions for 1 subreddit from start of day to end\n",
    "                for submission in subreddit.submissions(day_start,day_end):\n",
    "                    #json_row holds our row data\n",
    "                    json_row = {'title':None,\n",
    "                                'subreddit':None,\n",
    "                                'author':None,\n",
    "                                'create_date':None,\n",
    "                                'submission_id':None,\n",
    "                                'num_comments':None}\n",
    "\n",
    "                    #increment our total sub counter\n",
    "                    total_subs+=1\n",
    "\n",
    "                    #get all the data values wanted\n",
    "                    json_row['title']=submission.title\n",
    "                    json_row['subreddit']=team[0]\n",
    "                    json_row['author'] = submission.author.name\n",
    "                    json_row['create_date'] = submission.created_utc\n",
    "                    json_row['submission_id'] = submission.id\n",
    "                    json_row['num_comments'] = submission.num_comments\n",
    "\n",
    "                    #appends the json data row to list in totalday data. If extraction failed the values will be None    \n",
    "                    json_totalday['data'].append(json_row)\n",
    "\n",
    "\n",
    "            except:\n",
    "                #if extraction fails, create a list of 2 items, the string date and string name for subreddit\n",
    "                #and append it to json_error_catcher list\n",
    "                json_error = {'error':True,'team':team[0],'day':single_day.strftime('%Y-%m-%d')}\n",
    "                json_totalday['data'].append(json_error)\n",
    "                json_error_catcher.append([single_day.strftime('%Y-%m-%d'),team[0]])\n",
    "\n",
    "\n",
    "\n",
    "        #records the date\n",
    "        json_totalday['date'] = single_day.strftime('%Y-%m-%d')\n",
    "        #records total subs for day\n",
    "        json_totalday['day_count'] = total_subs\n",
    "\n",
    "        #if there was an error then append the errors to the 'errors' list in json_totalday for later inspection\n",
    "        #if no errors then json_totalday['errors] will be an empty list.\n",
    "        if len(json_error_catcher) > 1: json_totalday['errors'].append(json_row['error'])\n",
    "\n",
    "\n",
    "        json_totalseason['period_data'].append(json_totalday)\n",
    "        #print(single_day.strftime('%m-%d'),'done')\n",
    "\n",
    "    runtime = time.time() - program_start_time\n",
    "    json_totalseason['runtime']=runtime\n",
    "    print('COMPLETE')     \n",
    "    print(\"--- %s seconds ---\" % (runtime))\n",
    "          \n",
    "    return json_totalseason\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Runs the function and creates json files for each NBA season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 5.1.0 of praw is outdated. Version 5.2.0 was released Tuesday October 24, 2017.\n",
      "COMPLETE\n",
      "--- 1404.31862282753 seconds ---\n",
      "COMPLETE\n",
      "--- 1462.404468536377 seconds ---\n",
      "COMPLETE\n",
      "--- 584.7969391345978 seconds ---\n",
      "COMPLETE\n",
      "--- 1758.0287728309631 seconds ---\n",
      "COMPLETE\n",
      "--- 2044.4512968063354 seconds ---\n",
      "COMPLETE\n",
      "--- 682.819149017334 seconds ---\n",
      "COMPLETE\n",
      "--- 1501.9678287506104 seconds ---\n",
      "COMPLETE\n",
      "--- 2117.67236828804 seconds ---\n",
      "COMPLETE\n",
      "--- 796.2280328273773 seconds ---\n",
      "COMPLETE\n",
      "--- 1550.2451255321503 seconds ---\n",
      "COMPLETE\n",
      "--- 2323.0882914066315 seconds ---\n",
      "COMPLETE\n",
      "--- 787.9180357456207 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for year in season_dates:\n",
    "    for season_type in season_dates[year]:\n",
    "        test_dict = {}\n",
    "        test_dict = reddit_Submissions_toJSON(subreddits_list,season_dates[year][season_type])\n",
    "        test_dict['season_type']=season_type\n",
    "        test_dict['nba year'] = year\n",
    "        \n",
    "        with open('{}-{}.json'.format(year,season_type), 'w') as outfile:\n",
    "            json.dump(test_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "big_stitcher = []\n",
    "counter = 0\n",
    "\n",
    "for year in season_dates:\n",
    "    for season_type in season_dates[year]:\n",
    "        data = json.load(open('{}-{}.json'.format(year,season_type)))\n",
    "        for day in data['period_data']:\n",
    "            for row in day['data']:\n",
    "                big_stitcher.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556484"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big_stitcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('json_dump.json','w') as out:\n",
    "    json.dump(big_stitcher,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client=MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client.nba_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'nba_reddit'), 'main')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1b125070148>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.main.insert_many(big_stitcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5a15d2613a162e17e49f5b61'),\n",
       " 'author': 'schnide1',\n",
       " 'create_date': 1371884292.0,\n",
       " 'num_comments': 16,\n",
       " 'submission_id': '1gug4i',\n",
       " 'subreddit': 'LAL',\n",
       " 'title': \"Even Brazilian Laker fans want D'Antoni out...\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.main.find_one({'subreddit':'LAL'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
